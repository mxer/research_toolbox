











# TODO: add options to directly initialize a git repo.


### some useful inspection functions.
# NOTE: should I sort the sentences or just use them as they are.


# NOTE: this can be done differently and have options that determine if something 
# is done or not.
    # TODO: changes into the folder that I care about, runs something and 
    # returns to where I was before.
    # subprocess.call(["cd && "])

# TODO: I think that it would be nice to have some utility scripts that can 
# be directly added to the model.

# for example, scripts to download the data that you care about. 
# git ignore.

### string manipulation
def wrap_strings(s):
    return ', '.join(['\'' + a.strip() + '\'' for a in s.split(',')])

# string manipulation.

# TODO: something to split on spaces 
# or strip on spaces. and stuff

# join strings by separator.

# it is very useful to manage the output in such a way that it is easy 
# to send things to a file, as it is easy to send them to the terminal.
# 

# callibration for running things in the cpu.

# if there are multiple things to be loaded, it is best to just do one.
# or something like that.

# it would be nice if merging the keys would preserve some form of 
# ordering in the models.

# TODO: perhaps additional information that allows to run things on matrix 
# directly by syncing folders and launching threads.

# compress path? this is something that I can do conveniently.

# run on the server with multiple gpus and multiple experiments.
# easiest. split into multiple experiments.

# there is also the question about launching things in parallel, and 
# how can I go between Python and bash.

# NOTE: some of these things will not account for recursion in directory copying,
# not even I think that they should.

# send email functionality, or notification functionality. 
# sort of a report of progress made, how many need to be done more and stuff like 
# that.

### load all configs and stuff like that.
# they should be ordered.
# or also, return the names of the folders.
# it is much simpler, I think.

# stuff to deal with embeddings in pytorch.

# connecting two computational graphs in tensorflow.

# TODO: some functionality will have to change to reaccomodate
# the fact that it may change slightly.

### profiling


# the maximum I can do is two dimensional slices.

# get all configs.
# # TODO: add information for running this in parallel or with some other tweaks.
# def run_experiment_folder(exp_folderpath, ):
#     scripts = [p for p in list_files(exp_folderpath, recursive=True) 
#         if path_last_element(p) == 'run.sh'][:5]

#     print scripts
#     print len(scripts)
#     for s in scripts:
#         subprocess.call([s])
    
    ### call each one of them, it can also be done recursively.

# how to identify, them
# plot the top something models and see 
# problematic to represent when they have 30 numbers associated to them.
# pick up to 7 representative numbers and use those.

# top 10 plots for some of the most important ones.

# TODO: <IMPORTANT> standardization of the files generated by the experiment,
# and some functionality to process them.

### TODO: upload to server should compress the files first and decompress them 
# there.

# may comment out if not needed.

# for boilerplate editing

# add these. more explicitly.
# TODO: add file_name()
# TODO: file_folder
# TODO: folder_folder()
# TODO: folder_name()

# add more information for the experiments folder.
# need a simple way of loading a bunch of dictionaries.

# there is the question of it is rerun or dry run.

# basically, each on is a different job.
# I can base on the experiment folder to 
# with very small changes, I can use the data 
# data and the other one.
# works as long as paths are relative.

# this model kind of suggests to included more information about the script 
# that we want to run and use it later.

## NOTE: can add things in such a way that it is possible to run it
# on the same folder. in the sense that it is going to change folders
# to the right place.

# I think that I can get some of these variations by combining various iterators.

# a lot of experiments. I will try just to use some of the 
# experiments.

# running a lot of different configurations. 
# there are questions about the different models.
# 


# TODO: need to add condition to the folder creation that will make it 
# work better. like, if to create the whole folder structure or not.
# that would be nice.

# needs to create all the directories.

# needs to create the directories first.

# assume that the references are with respect to this call, so I can map them 
# accordingly

# stuff to kill all jobs. that can be done later.
# <IMPORTANT> this is important for matrix.

# NOTE: this may have a lot more options about 
def run_experiment_folder(folderpath):
    pass

# there needs to exist some information about running the experiments. can 
# be done through the research toolbox.

# assumes the code/ data 
# assumes that the relative paths are correct for running the experiment 
# from the current directory.

# this can be done much better, but for now, only use it in the no copying case.
# eventually, copy code, copy data depending on what we want to get out of it.
# it is going to be called from the right place, therefore there is no problem.


# even just upload the experiment folder is enough.

# TODO: stuff for concatenation. 

################################### TODO ###################################
# ### simple pytorch manipulation
# NOTE: this needs to be improved.
# import torch

# # TODO: with save and load dicts.
# def pytorch_save_model(model, fpath):
#     pass

# def pytorch_load_model(fpath):
#     pass



# have to hide the model for this.

# time utils

# tensorflow manipulation? maybe. installation of pytorch in some servers.

# call retrieve_values with locals()

# massive profiles.

# TODO:
def wait(x, units='s'):
    pass
    # do something simple.

# TODO: do a function to collapse a dictionary. only works for 
# key value pairs.

# add the ignore hidden files to the iterator.

# stuff to read models from the file, but serialized.

# stuff for checkpointing. resuming the model. what can be done.
# stuff for running multiple jobs easily.

# stuff for working with PyTorch.

# along with some interfaces.

# keep the most probable words.

# some of these are training tools. 
# this makes working with it easier.

### dealing with text.

# some default parameters for certain things.
# this is also useful to keep around.

# check if I can get the current process pid.

# managing some experimental folders and what not.

# will need to mask out certain gpus.

# probably, it is worth to do some name changing and stuff.

# these are mostly for the servers for which I have access.

# may need only the ones that are free.

# perhaps have something where it can be conditional on some information 
# about the folder to look at.
# If it is directly. it should be recursive, and have some 
# form of condition. I need to check into this.

# information on what gpu it was ran on. 
# check if some interfacing with sacred can be done.

# call these these functions somewhere else. 
# this should be possible.

# do some well defined units;
# have some well defined server names.

# creates the wrappers automatically.

# needs to be updated.

# folder creation and folder management.

# mostly for reading and writing, since I think it is always the same.
# I think that it should get a single figure for it.
# depending on what you pass, you should be able to get what you want.


# I don't think that I will need it.

# simple interfaces for models.

# for preprocessing
# for loading data
# for models
# for evaluation
# for visualization.

# do some more error checking for the writing the dictionaries to disk.
# like, no new lines

# make them strings, for ease of use.

# TODO: tools for managing a log folder automatically

# name experiments; if not, just have a single folder where everything is there. 
# sort by some date or id.

# in some cases, operate directly on files.
# take a list of files, and return a csv.

# wrap certain metrics to make them easy to use.

# prefer passing a dictionary, and then unpacking.

# something for creating paths out of components

# TODO: stuff for using Tensorflow and PyTorch and Tensorboard.

# better dictionary indexing and stuff like that.

# that can be done in a more interesting way.

# there are probably some good ways of getting the configurations.
# ideally, I think that it should be possible to sample configurations 
# uniformly.


# for tied configurations.
# basically, in cases where we want special conbinations.

# gives back, argnames, argvals.


# NOTE: there is probably something analogous that can be done for iterators.


# dictionary aggregate, test what are the value that each element has 
# it only makes sense to have such a dictionary for extra error checking.

# also, iteration over the folders that satisfy some characteristics, 
# like having certain values for certain parameters.

# also options about if there are two with the same parameters,
# I can always get the latest one.

# the experiment folder should be self-contained enough to just be copied to some
# place and be run there.
# resarch toolbox would be accessible through that folder,

# "import research_toolbox as tb; tb.run_experiment_folder("exp0", )

# these iterations cycles are simple, but take some time to figure out.

# depending on the dimensions.
# checking these configurations is interesting.

# do something that goes directly from DeepArchitect to a figure.

# something between interface between DeepArchictect and a Figure.

# add common ways of building a command line arguments.

# add common arguments and stuff like that.
# for learning rates, and step size reduction.

# for dealing with subsets, it is important to work with the model.

# do some structures for keeping track of things. easily.
# perhaps just passing a dictionary, and appending stuff.
# dealing with summary taking it is difficult.

# files to write things in the CONLL-format 

# I'm making wide use of Nones. I think that that is fine.
# keep this standard, in the sense that if an option is not used,
# it is set to None.

# stuff to make or abort to create a directory.

# iterate over all files matching a pattern. that can be done easily with re.

# simple scripts for working with video and images.

# the whole load and preprocessing part can be done efficiently, I think 
# I think that assuming one example per iterable is the right way of going 
# about this. also, consider that most effort is not in load or preprocessing
# the data, so we can more sloppy about it.
    

# NOTE: there are also other iterators that are interesting, but for now,
# I'm not using them. like change of all pairs and stuff like that.

# NOTE: numpy array vs lists. how to deal with these things.

# do some random experiment, that allows then to do a correction,
# for example, given 

# maybe passes on a sequence of tests that are defined over the 
# experiments. 
# this means that probably there is some loading of experiments 
# that is then used.

# set_seed
# and stuff like that. 
# it is more of an easy way of doing things that requires me to think less.

# copy directory is important
# as it is copy file.

# copy directory. all these can be done under the ignore if it exists 
# or something like that. I have to think about what is the right way of 
# going about it.

# create these summaries, which make things more interesting.

# managing these hyperparameter configurations is tedious. there is 
# probably a better way that does not require so many low level manipulations.

# problem. something in other languages will not work as well.
# does not matter, can still do most of it in Python.

# exposing the dataset. soem form of list or something like that.
# doing that, I think that it is possible to implement some common 
# transformations.

# can do some form of automatic collection, that gets everything into a file.
# the creation of these configuration dictionaries is really tedious.

# TODO: something for profiling a function or script.
# open the results very easily.

# TODO: add typical optimization strategies, such as how to do the 
# step size and how to reduce it. to the extent possible, make it 
# work on tensorflow and pytorch.

# make it easy to run any command remotely as it would locally.
# remote machines should be equally easy to use.

# look the DeepArchitect experiments to see what was done there.

# also, a lot of file based communication, like, you can generate the 
# output directly to a file.s

# add more restrictions in the keys that are valid for the dictionary
# I don't think that arbitrary strings are nice.

# another way of doing things is to write the names and capture 
# the variables from the local context, that way, I only have to write the names, 
# i.e., it becomes less tedious.

# functions to send emails upon termination. that is something interesting,
# and perhaps with some results of the analysis.

# also, it is important to keep things reproducible with respect to a model.
# some identification for running some experiment.

# plotting is also probably an interesting idea.

# TODO: logging can be done through printing, but there is also
# the question about append the stdin and stderr to 
# the file or terminal.

# also, potentially add some common models

# have an easy way of generating all configurations, but also, orthogonal 
# configurations.

# generate most things programmatically.

# some things can be done programmatically by using closures.
# to capture some arguments.

# also, given a bunch of dictionaries with the same keys, or perhaps 
# a few different keys, there are ways of working with the model directly.

# can have a few of them that go directly to a file.
# writes the configurations directly to a file.

# experiment id or something like that. 
# just the time and the commit.

# also, do things for a single row.

# get things out of a dictionary. I think that it should also be possible 
# call a function with the arguments in a dictionary.

# just the subset that it requires, I think that it is fine.
# it would be nice to pass extra stuff without being needed, but still 
# wouldn't have the model complain.

# important to have these auxiliary functions.

# probably those iterators would like to have a dictionary or a tuple
# that would be nice.

# essentially, question about using these paths, vs using something that is more 
# along the lines of variable arguments.

# working on the toolbox is very reusable.

# more like aggregated summary and stuff like that.

# especially the functions that take a large number of arguments are easier 
# to get them working.

# check if PyTorch keeps sparse matrices too? probably not
# Why do I care?

# stuff that is task based, like what wrappers and loaders, and preprocessors
# and stuff like that.

# stuff for image recognition, image captioning, visual question answering

# all these setups still stand.

# probably has to interact well with dictionaries. for example, I could 
# pass an arbitrary dictionary and it would still work.
# also should depend on the sequence, 

# and it may return a dictionary or a tuple. that seems reasonable enough.

# manipulation of text files is actually quite important to make this practical.
# tabs are not very nice to read.

# NOTE: going back and forth using these dictionaries is clearly something that 
# would be interesting to do easily.

# some capture of outputs to files.

# something to sample some number uniformly at random from the cross 
# product.
# it is simple to construct the list from the iterator

# better do at random and then inspect factors of variation.
# the most important thing is probably time and space.

# use the principles about being easy to try a new data set. 
# try a new method, try a new evaluation procedure
# try a model
# this is perhaps hard to do.
# maybe it is possible to develop functions that use that other functions 
# and sort of do that.
# like fit_functions.

# communication and returning stuff can be done through dictionaries and stuff like 
# that.

# logging is going to be done through printing.

# either returns one of these dictionaries or just changes it.

# be careful about managing paths. use the last one if possible, t

# stuff for string manipulation

# the summaries are important to run the experiments.

# simple featurizers for text, images, and other stuff.

# dictionaries are basically the basic unit into the model.

# another important aspect is the toolbox aspect, where you 
# things are more consistent that elsewhere

# creating command-line argument commands easily

# TODO: more stuff to manage the dictionaries.

### another important aspect.
# for creating interfaces.
# actual behavior of what to do with the values of the arguments
# is left for the for the program.
# use the values in 

# may specify the type or not.

# generating multiple related configs easily.
# perhaps by merging or something like that.
# and then taking the product.

# NOTE: may there is a get parser or something like that.

# easy to add arguments and stuff like that. what 
# cannot be done, do it by returning the parser.

# may be more convenient inside the shell.

# # get the output in the form of a file. that is good.

# get all available machines 
# TODO: this may be interesting to do at the model.

# this is especially useful from the head node, where you don't have to 
# login into machines.

# getting remote out and stuff like that.

# run on cpu and stuff like that.
# depending on the model.

# can also, run locally, but not be affected by the result.

# it works, it just works.

### some simple functions to read and write.

# get current commit of the model.

# stuff to handle dates.

# if it is just a matter of tiling the graph, it can be done.

# can either do it separately r 

# TODO: add stuff for easy hyperparameter tuning.
# it is easy to make the code break.
# also, check some of the simple stack traces up front.


# GridPlots and reference plots.

# all factors of variation.

# class ReferencePlot:
#     pass
# the goal is never have to leave the command line.

# scatter plots too. add these.

# folder creation

# TODO: generating a plot for a graph.
# TODO: generating code for a tree in tikz.

### for going from data to latex

# NOTE: no scientific notation until now.
# possibilities to add, scientific notation.
# to determine this, and what not.

# probably in the printing information, I should allows to choose the units
# TODO: the table generation part was not working OK yet.
# some problems, with bolding the elements 

# perhaps generate the table directly to a file.
# this works best and avoids cluttering the latex.

# maybe have other sequences by doing a comma separated list.
# mostly for single elements.
# used for setting up experiments.

### TODO: perhaps make this more expressive.

# all should be strings, just for ease of use.

# information about the single run can be generated somehow.
# maybe not in the results, but in some form of log.

# ignoring hidden folders is different than ignoring hidden files.
# add these two different options.

# perhaps ignoring hidden files vs ignoring hidden folders make more sense.

# can I do anything with remote paths to run things. I think that it may 
# be simpler to run models directly using other information.

# if no destination path, in the download, it assumes the current folder.
# TODO: always give the right download resulting directory.

# probably more error checking with respect to the path.

# TODO: perhaps will need to add ways of formating the json.

# NOTE: add support for nested columns.

# add support for grids of images.

# DO a conversion from a dict to an ordered dict
# TODO: something that makes it easy to generate different graphs.

# stuff for profiling and debugging.

# should be easy to compute statistics for all elements in the dataset.

# also, adopt a more functional paradigm for dealing with many things.
# all things that I can't do, will be done in way  


# using the correct definition is important. it will make things more consistent.

# even for simpler stuff where from the definition it is clear what is the 
# meaning of it, I think that it should be done.

# don't assume more than numpy.
# or always do it through lists to the extent possible.

# use full names in the functions to the extent possible

# use something really simple, in cases where it is very clear.

### this one can be used for the variables.

# json is the way to go to generate these files.

# decoupling how much time is it going to take. I think that this is going to 
# be simple.

### NOTE: this can be done both for the object and something else.

# it they exist, abort. have flag for this. for many cases, it make sense.

# important enough to distinguish between files and folders.

# perhaps better to utf encoding for sure.
# or maybe just ignore.

# for download, it also makes sense to distinguish between file and folder.

# for configfiles, it would be convenient.

# there is code that is not nice in terms of consistency of abstractions.
# needs to be improved.

# managing the data folder part of the model. how to manage folders with 
# data. perhaps register.

# add some stuff to have relative vs direct directories.
# get something about the current path.
# get information about hte working directory.

# running different configurations may correspond to different results.

# useful to have a dictionary that changes slightly. simpler than keeping 
# all the argumemnts around.
# useful when there are many arguments.

# run the model in many differetn 

# transverse all folders in search of results files and stuff like that.
# I think path manipulation makes a lot more sense with good tools.
# for total time and stuff like that.

# this can be useful to having stuff setup.

# stuff to pull all the log files from a server.
# stuff to easily generate multiple paths to ease boilerplate generation.
# for example, for generating long list of files to run in.

# for example, read a file that is named similarly from 
# all the elements of a folder. that file should be a dictionary.
# the question there is that it should probably json for ease of 
# use.

# apply a function to each of the elements, or may be not.

# that is an interesting way of getting the results from the model.

# can capture all the argumentss, I guess, which means that 
# that I can use the information.

# next thing: I need to make this more consistent.

# TODO: very important, check the use of the models that I need to

# merge multiple parsers.

# make it possible to merge multiple command ilne arguments.

# stuff to append. stuff to keep an average 
# and stuff like that.
# stuff to do both.

### all files matching a certain pattern. 
# 

# function to filter out. 

# doing the same thing in multiple machines.

# more stuff to deal with how to iterate over directory folders.

# can do a lot more for registering stuff.

# needs some more tools for managing directories.

# needs to improve stuff for reproducibility.

# needs to improve stuff for reproducibility.
# generating rerun scripts assuming data is in a certain place. 

# stuff for managing experiment folders.

# default configurations for running all, and running the ones we need.

# all structure to include in the generation script in there.
# this could be the general structure of a repo. 

# generate simple scripts for it. they should be able to get the absolute paths
# to the data.

# passing the data directory is good form for running the code.

# also, interactive filtering, perhaps with a dictionary, or something 
# like that. 

# running them from the command line makes more sense.

# basically, each task is calling the model some number of times 

# need to have a way of calling some python command on the command line 
# there and returning the results.

# this will return the results 

# some python command from some lib, with some addional imports.
# this will be the case. which can actually be quite long, 
# that would be quite useful actually.

# stuff for managing experiments necessary.

# add some simple function to retrieve information from the model.
# like time and other stuff.

# for directly generating experiments from main and stuff like that.

# get paths to the things that the model needs. I think that it is a better way of
# accomplishing this.

# analysis is typically single machine. output goes to something.

# copy_code.

# code in the experiment.
# stuff like that.

# for getting only files at some level

# TODO: creation of synthetic data.

# do the groupby in dictionaries
# and the summaries.

# should be simple to use the file iterator at a high level.
# produce the scripts to run the experiment.

# decide on the folder structure for the experiments... maybe have a few
# this is for setting up the experiments.

# I'm making a clean distinction betwenn the files that I want to run and 
# the files that I want to get.

# stuff to process some csv files.
# stuff to manipulate dates or to sort according to date. 
# perhaps create datetime objects from the strings.

# the question is sorting and organizing by the last run.
# it is also a question of 

# will probably need to set visability for certain variables first.

# the goal is to run these things manually, without having to go to the 
# server.

# most of the computational effort is somewhere else, so it is perfectly fine 
# to have most of the computation be done somewhere else.

# have a memory folder, it is enough to look for things. 
# can create experiments easily.

# having a main.
# it is possible to generate an experiment very easily using the file.
# and some options for keeping track of information.

# this can be done to run the code very easily.
# taking the output folder is crucial for success.

# if the code is there, there shouldn't exist really a problem with getting it to 
# work.

# this may have a lot of options that we want to do to get things to 
# work. maybe there is some prefix and suffix script that has to be ran for these
# models. this can be passed somewhere.

### TODO: other manipulation with the experiment folder.
# that can be done.

# it should copy the code without git and stuff. actually, it may be worth 
# to keep git.

# there are folders and subfolders. 

# may exist data_folderpath or something, but I would rather pass that 
# somewhere else. also, maybe need to change to some folder.

# TODO: probably do something to add requirements to the code. 
# this would be something interesting.

# run one of these folders on the server is actually quite interesting.
# I just have to say run folder, but only every some experiment.
# that is what I need.

# operating slurm form the command line.

# this is going to be important for matrix and the other ones.

# login
# change to the right folder
# call the correct function to run the experiment
# (another fucntion to get the results.)

# it really depends on what folder it should work 
# with. there is a question about what kind of running stuff will it work 
# with. it is really tricky to get it to run on multiple machines.

# it always refer within the data folder. this is the right thing 
# to do.

# create an experiment folder that is sort of on par.

# also, regardless of the path used to run the things, it should 
# always do the right thing..

# available memory per gpu and stuff like that.

# probably I should have some way of asking for resources.

# maybe wrap in a function.
# gpu machines in lithium.

### will need some configs for slurm. how to set things up.

# how to work with long running jobs and stuff like that.
# I need some status files for experiment folders.

# completing the task, would mean that the model, 

# for periodic saving things. 

# just need some path, the rest is agnostic to that path.
# just needs to know the path and that is it.

# simple jobs for now. and I will be happy.
# multiple runs for the project.

# readme json with other information

# may just find a new name.

# always ask for a name.

# simple naming conventions
# typical folder would be experiment.

# can be local or it can be global
# run the folder.

# have some tools for padding.
# it may have a way of doing reentry, or it may simply try different models.

### NOTE: I need stuff to get the configs out of the folder of the model.
# what else can we get from it.

# generates a run script and stuff like that.

# no defaults in the experiments.

# copy folder, but with current code. something like  that.

# to manipulate this folder, it is necessary to do the configs.

# programs need to be prepared for reentry.

# about experiment continuity and reentry.
# that needs to be done with the toolbox.

# look for files of a certain type.

#### ****************
# there are questions upon the creation of the model
# run.sh all (run fraction, )  
# out.txt
# config.json
# results.json
# .... (other files) ....
# main has to be prepared for reentry, that is it.
# and files have to be appended.

# needs easy way of making the code run directly. never know anything,
# for example, how does it know that it has been finished or not.

# do it all in terms of relative paths.
# or perhaps map them to non-relative paths.

# can add more configurations to the experiments folder.

# assumes that the config carries all the necessary information.

# assumes that the model knows what to do with respect to the given data path.

# it is a question of reentry and checkpoints and stuff. it runs 
# until it does not need to run anymore.
# perhaps keep the model.

# stuff that I can fork off with sbatch or something.

# get a subset of files from the server.
# that is quite boring to get all the information from the files.

# there is stuff about making parents and things like that.

# temp, out and other things that are more temporary and for which your command
# does not care too much.

# either all or a fraction.

# working with path fragments and stuff like that.

# commit on the structure.

# reading the results is going to be custom, but should be simple.

# the should exist download tools that mimic the directory structure, but only 
# get some of the files.
# needs to add some more information to the model.

# --parallel
# does one after the other, or something like that.

# list of modules to load and stuff like that. 
# I think that the experiment was done for the machine.
# after having all those things, there is no need of the research toolbox.

# TODO: I need stuff for GPU programs.

# relative paths are better.
# I don't see a reason to use absolute paths.
# works better in the case of copying to the server.

# the other aspect is visualization.
# that is quite important after you have the results.

# most commands are going to be submitted through the command line.
# it would be convenient to run the experiment creation in the model.
# that is something while the other part of the mdooe l 

# copy the results of the model to my persistent space.

# timing based on my iteration.
# that is something that is quite interesting.

# the fast config to run locally.
# I think that it would make sense to run those experiments.
# it should probably have some form of configuration itself that 
# is useful in reading the model.

# what can I do with that model. this is actually quite interesting. 

# NOTE: type of functions that can be considered to explore 

# for gpu programs and stuff like that
# maybe I could add options which are common in generating the bash scripts 
# such as what is the model doing and stuff like that.

# NOTE: it does not have to be directly this part of the model.
# it has to be some form of the model that should work directly.

### I will need to do something for working with this type of model
# in C or something like that. the question is that I need a few functions 
# to work with the model. 

# number of jobs. they can be ran in parallel, but they can also be ran in series.
# so why split. to have more variability in the results before getting 
# anything. 

# slurm remote runs, how to do it. how can I get it to work.
# remote slurm vs other type of slurm.

# the tools are important in getting the right amount of memory from the 
# model. for example, I model it quite easily. then, I can ask for 
# two times the necessary memory.

# do stuff with the scheduler.

# featurization would be convenient. 
# like some form of adding features. maybe, as a named dictionary 
# or positionally.
# these specifications are really boring, but they are necessary.
# there is really not much to do here.

# can be a function from the toolbox. 
# just creates the path as I want it.

# TODO: add the stuff for graph viz in both pytorch and tensorflow.
# the problem is that there are things that I need to care about.

# graph visualization can be crucial for understanding what is happening 
# in the model. it should be possible to wrap the model easily.

# using hooks to access intermediate gradients.

# TODO: can introduce some easy ways of testing the program? 
# it is hard to do this in general. perhaps in some narrow domain specific way 
# it is possible.

# experiments with sub experiments.

# it is better to run experiments from a single place, using the toolbox,
# and the config. 
# it seems annoying to have the config and the other ones.
# can lead to inconsistencies.

# allow for sub experiments.

# TODO: add support for nested experiments.
# I think that this is possible simply by having the right thing.
# these processes can be simple.

# example, a config that runs some number of other configs for some other 
# experiment.

# do things for the exploration of csv files. NOTE: can be done through pandas.
# this is going to be very handy to explore the results.

# easy way of fixing some of the arguments.

# NOTE: some of the stuff for connecting graphs in tensorflow.

# TODO: once it grows to a certain range, it will be useful to create toolbox 
# files mimicking some of the evaluation metrics that we looked at.
# this makes sense only after reaching a certain number of things to 
# do, or having libraries that are not guaranteed to exist in the model.

# TODO: for debugging, I can add information about the calling stack or 
# the function that is calling some methods. this may be useful for logging.
# can be done through inspect.

# questions about the model.

# stuff to manipulate existing experiments folders.
# working in terms of relative paths is crucial for this functionality.
# for example, generating scripts for running the code in a certain way

# add functionality for relative directory manipulation.

# TODO: in some cases, it is possible to output things to terminal too.
# this means adding some tee to the model.

# to get the full encapsulation, I will need to do something else.
# this is going to be interesting.

# decrease by when and how much. question about the different part of the 
# model. this is an interesting question.

# add dates to the experiments, or at least make it easy to add dates.
# this can be done in main_draft.

# stuff for handling more complicated data, such as data bases and other type 
# of data.

# it may be worth to introduce information about some computational resources.

# some thing for, if a certain file exists, ignore, or somethigng like that.

# TODO: add the download of only files of a certain type.'

# add ignore lists to file transfer and what not. 
# it is better to compress before transfering, probably.

# look into rsync. this is something that might be interesting.

# TODO: add the information about the model 

# using stuff in slurm, this is going to be important.

# there is definitely the possibility of doing this more seriously for using 
# slurm commands. the question is how far can I take this.

# tools for taking a set of dictionaries, and sorted them according to 
# some subset of the keys. this can be done easily. 
# manipulating lists of dictionaries should be easy.

# TODO: all the sorting elements must exist.

# it would be interesting to have some form of interrupts that allow a 
# process to stop, or to call a function with some time limit, but 
# to terminate it if necessary.

# the important thing is that there are multiple models that can be used.
# for example, it can be possible to try different models and check that it is 
# going to work in certainn way.

# make  it easy to run some of the search algorithms in parallel or something 
# like that. what can be done? it is important to set up a database or 
# have a central thread that takes care of this. interprocess communication
# to do this. very simple code.

# there is also stuff to do the analysis of the model.

# handling different data types very easily. this is going to be something 
# interesting.

# the main components are data and model. add some functionality do 
# deal wtih both of them.

# abide by some interface where it possible to pass some logging information
# using some consistent interface to the logging functionality, and just pass
# that around. 
# that would be important for getting things to work properly

## TODO: perhaps I can add information about how to make this work. 

# important for visualization of mistakes of a model. how to inspect the mistakes?
# perhaps write a json, or just write some form of function that takes
# some performance metric, and write all examples and predictions for 
# visualization.

# add a condition to test if running on the server or not. this 

# check if running on the server or not.

# maybe the best way of doing the import of matplotlib is to do it, is to do a 
# try and raise.

# TODO: tools for checking that all the experiments are complete, and perhaps 
# for removing the ones that are not complete.
# there is somemore additional processing of results that would be interesting.

# add more functionality to streamline profiling. it should be easy to extract
# relevant information. look at the pstats objects.

# clearly, the question of running multiple jobs on lithium is relevant.
# look into this. use a few gpu machines.

# add scripts for dealing with slurm and the other one.
# interface by having simple interfaces that generate the scripts.
# such it is simply doing srun on the script.

# TiKz figures. easy to manipulate. for simple cases. e.g., block diagrams 
# or trees. getting some consistency on the line width would be important.
# how can I do a more consistent job than the other one.

# do some more of this for some simple bash scripts.

# TODO: have some way of representing attention over the program given 
# the error message and perhaps some more information. this is going to be 
# important. the other question is instrumentation. 
# if I have a small dataset, keeping the field names with it is not so bad.

# also, if the dataset is a sequence of time events, that is kind of trickier
# to collect. or is it? maybe not.

# datasets collected as soon as possible, or collected incrementally.
# define fields and keep adding to them. serialization through json.
# loading through json.
# or perhaps csv. depends on the creation of a new file.

# take a script and make it some other type of script. example, 
# from running locally to running on the server.

# do the experiments in both the cluster and here.

# have some utility scripts that can be called from the console.
# also, perhaps, some way of easily creating some files and iterating over files.
# it is important to work easily with them

# module load singularity 
#   exec centos7.img cat /singularity

# there are only a few that 

# it is easy to preappend with what we have done.

# something for wrapping the experiments, I guess.

# def make_slurm_with_tf(exp_folderpath):
#     "module load singularity"
#     "singularity run %s" % exp_folderpath

# problems defining some of these scripts. version is too 

# there is probably some way of redirecting output.

# to run scripts that input, it is 

# generating easily the scripts for running on the machine. do not worry about 
# having too much stuff now.

# TODO: I will need some form of managing experiments across projects.

# handling the experiments some how. this would be possible easily.

# easy to send things to server. 
# should only take a few minutes.

# make a script to install some of these things easily.
# this will make setting up easier.

# sort of incremental contruction of the dictionary which is constant
# pushed to disk
# write to pickle file.

# add a command to call bash commands.
# this is going to make it easier getting access to some of information 
# from the command line.

# write a lot of reentrant code. make every cycle count.

# set up the repos; run more experiments for beam_learn; do more stuff.

# do different commands for running in different servers there.
# it should be possible to run a command directly after starting a session there.

# NOTE: remember that it is nice to run this thing from command line. 
# there is still the question about creating these things through the command 
# line. the question is how to manage multiple functionalities.

# NOTE: for having something about key identification for not having a 
# password prompt for connecting to the server.

# TODO: add some functions to manipulate csv files to generate tables about 
# quantities that I care about.

# add the results.json file.
# file. write json file with just the fields that matter.

# TODO: handle better the continuum of steps that can be taken. 
# this means 

# TODO: tests for whole experiments in the sense that is mostly about what do we 
# expect to happen and why is it not happening. it is just going to help going 
# through the experiments the first time.

# TODO: the project should be easy to manage. it should be possible to 
# read the data from disk, do something and then, put it back on disk.

# map a file line by line somehow.

# TODO: have mailing functionalities upon termination of the job.
# this sounds something reasonable if I want to check the results soon.

# NOTE: for experiments, it is important to maintain consistency between 
# the remote version and the local version.

# TODO: have a way of syncing folders

# possible to have character escaping there.

# TODO: there is the question of on which compute node was it ran. 
# that can be done through sacred, if I really want to be 

# TODO: functions for file processing. like taking a file with a huge number of 
# functions and just keeping those that I care about.
# this is clearly processing the file. and then, keep only the imports 
# used at the top. NOTE: that there is some form of dependency because some 
# auxiliary functions may use some other auxiliary functions, 
# there is some recursion that I need to go about.

# TODO: make it easy to run callibration experiments to get a sense of how 
# much an epoch is going to take. also, I think that it would be interesting 
# to have some form to actually talk about epochs and performance. that would 
# make it easier.

# NOTE: have a simple way of running any command of the toolbox on the command
# line. I think that this is only possible for commands with string arguments 
# or I have to anottate the commands with tags.
# --int --str --float 
# --bool is currently unsupported

# these are going to especially useful for file manipulation, and for file 
# file creation.

# TODO: I think that it would also be interesting to have a way of running a 
# folder somehow. 

# extracting the requirements from running some command

# manipulation of the model.

# TODO: run functions from the command line. it can also list all functions.
# not all are callable.

# TODO: check how to fix the randomness for things such as tensorflow 
# and other problems.

# comparison of different experiments is going to be important, I think that 
# this is going to be mostly in the analysis code for plot generation and 
# other things.

# NOTE: preprocessing is going to be quite interestin, but I need 
# to be careful about how do I detect that something is a function call.

# TODO: there is probably some more interesting prints, but I do not 
# know exactly what do I want.

# to match a function call, basically, the name of the function followed by 
# a parens.
# can first parse the definitions and do that. Note that this is important 
# for generating the toolbox in a way that I'm happy sharing.
# also perhaps for reducing the size of file.

# TODO: something to remove all comments of a file.

# TODO: something to call from the command line to strip a file of comments 
# somehow. 
# in the case of python, perhpas move all imports to the top.
# NOTE: have a simple way of doing this processing in a file.
# NOTE: it is good to have something that allows me to go quickly and seamlessly 
# between the terminal and the python interpreter.

# NOTE: something to consider would be

# another interesting aspect is having something to remove all comments, or 
# just comments outside functions.

# the file manipulation part can be used effectively outside the shell.
# one high level goal is to have most of the operations be done outside the 
# function.

# TODO: there is stuff on data loading and data buffering that would be 
# interesting to consider. I don't know what are the main aspects that make 
# it possible to do it? is it just multithreaded code? check this out.
# have stuff that allows you to easily set up a model in different languages.

# TODO: perhaps also make it easy to submit jobs from the command line, 
# even it requires creating other things.

# TODO: develop tools for data collection. this means that I can register 
# information and stuff like that. 
# event data and stuff like. register certain things. simple data types.

# also, have th capacity of going seamlessly between disk and memory.

# for example, it should be possible to do everything with the experiments
# regardless of how much time passed between they have been created.
# failing to do this, means that you cannot reuse the information there. 

# TODO: provide a clock object that allows you do get time and date.
# and both in 

# TODO: study epoch and convergence curves and see what is the distribution
# of performance for the models. 

# TODO: it should be as simple to keep a dataset, and it is to keep code.
# data should be easy to generate and maintain. 

# TODO: data should be easy to change, and keep in place. data is effectively 
# the most important thing in ML.

# TODO: easy to crawl data and tools. how to?

# these in memory datasets are quite powerful, as we could read them and 
# write them to memory again. the append mode is better, but the
# question about keeping data is quite important. perhaps git works alright
# for simple things, but it is suboptimal. log data or event data.

# NOTE: the definition of data is essentially tied to a reader and writer, 
# and some form of registering stuff.

# training for longer or having longer patienties seems to make a difference 
# in running the models. 

# TODO: have some typical configs, like calibration, fast, noop, default.
# calibration is an experiment, rather than a config, note that a run is not 
# independent of the machine it was ran in.

# TODO: it should be possible to very easily extract a few quantities from 
# log files, do this. lines that do not match the pattern can be ignored.
# notion of a sequence of objects of the same type that can be processed easily.
# extracting information from files is very important.

# TODO: extracting information 

# TODO: perhaps I can do different tracking of the learning rates, or of 
# what constitutes an improvement. I think that asking for the best is a little
# strict. I think that it is better to keep a running average.
# if overall is improving, I think that it is better. maybe be less aggresive.

# TODO: it should be easy to serialize models and load them again. 
# it should be easy to run models on default data.

# typical data:
# - sequences
# - images
# - paired data
# - sequences of floats.

# it would be interesting to define these data types and work directly with them
# it would make it easier to do certain things.

# main preprocess, main train, main evaluate, main analyze 
# main all or something.
# should be easy to run all these things  
# what is the advantage of doing this?

# should be easy to run a sequence of these one after the other.
# it should be easy to keep a model around for future inspection.

# it should be easy to run a model on some data, just from the command line.
# I think that one the the main things is 

# extract data for graphs. 

# extract data from tree structures, like the simple deeparchitect with single 
# input, single output

# ability to interface with simpler code in C or something like that.
# that would be nice to do.

# have a research toolbox in C. for simple operations and simple data 
# structures. stuff like that.

# NOTE: perhaps the creation of certain code in C can be done through 
# Python, or at least some aspects can be generated, like functions, 
# argument types, and perhaps memory manipulation. 
# other interfaces that make sense can also be done there. I don't think 
# it is very easy to generate code in python for C. 

# TODO: interface for a model and a model trainer.

# TODO: have easy functionality for conditional running, e.g., if a file exists
# or not.

# if the operations are only a few, maybe the generation of the code can be 
# done in Python, otherwise, it seems hard to do. For example, if the 
# operations are mostly about doing some arithmetic operations, then it 
# should be possible to do them in Python, and define the data structures 
# accordingly.

# everything should be possible to do programatically. 
# the question is what is the benefit of doing this way rather than just doing 
# it. I think that 

# TODO: have a function to clear a latex file from comments. useful for submissions.
# careful about licenses. maybe be necessary to keep some licenses in some files.

# interface for data.

# interface for different tasks for putting together a model really fast.
# some general formats. 

# TODO: doing fine-tuning should be easy.

# TODO: have a sequence of patiences or something like that, or only tune a 
# few hyperparameters. 
# TODO: easy interfacing with DeepArchitect for setting search spaces.
# easy to train a few models based on some input representation.

# interface to the data should be easy.

# simple featurizer for sequences. 

# TODO: what about sharing the weights.

# TODO: always need to think about what is it going to be the representation for 
# the base models. doing this right is important to save computation. it si 
# better to keep the data in that format.

# TODO: models from graph data. 
# the question is the minimum information that I can use about the model.
# interface for setting up an easy LSTM model and easy Convolutional Net.
# these models should be super easy to use and load.
# the definitions and simple. the training should also be simple.

# I think that often the most important thing is just training for long.

# TODO: for preprocessing code files, have a way of sorting the function 
# definition by name or by number of args or whatever. number of times it 
# appear somewhere.

# it should be easy to have a folder and do something with it. for example,
# get the data at the end of the experiment. 

# tensorflow model manipulation. 
# pytorch model manipulation.

# make sure that you can address pytorch pitfalls.

# TODO: the server should feel like an extension of the local machine in 
# that it is simple to push things to the server and pull them from the server.
# it is easy to ask the server about how it is going.

# TODO: have an easy way of setting up email reminders when experiments 
# complete. do periodic emails with the state, and how are things going.

# TODO: minimum number of reductions before next reduction. this is 
# for the step size patience and stuff like that. it should help running things.

# easy error analysis, like  what are some of the examples that failed.

# the inspect module is nice for code manipulation.

# it should be easy to take a folder and run it on the cluster with some 
# configuration, then it should be easy to get the results back, maybe with
# locking or not.

# TODO: train something that knows when to reduce the step size of the models 
# based on the current step size, and the amount of variation observed.
# these should ideally be trained for a single model.
# there is also a question about normalization and stuff like that.

# TODO: add some preprocessors for different types of numbers. basically, 
# things that can be computed after you register all the training data.

# experiment sequencing is important, but I don't know exactly how to do it.
# it may be inside the cfg for that experiment, or it may be outside the experiments.
# it may be easier because it shows the dependencies, but I think that it 
# becomes too fragmented. if I have functions to go over all folders,
# I think that it is doable, but it is still trouble some.

# TODO: what can we do to tune the step size parameter?
# TODO: what to do when there? it is kind of like some form of optimization.

# TODO: how to deal with things where observations are sequences? I think that 
# it can be consumed with an LSTM. 

# TODO: implementation of simple hierarchical LSTM, maybe a single level or 
# something like that.

# I want it to be really easy to take something and just try something 
# TODO: perhaps add tools to make the setup easy too.

# run some of those models.

# add some additional conditions that help to detect if the model is training 
# or not.

# add support to some other plotting libraries, like d3 and other relevant 
# ones. 
# there is also graphviz that may be interesting for certain aspects of the code.

# TODO: add some default model optimization that uses some reasonable decrease 
# schedule, or tries a reasonable set of hyperparameters for the typical things.
# it is going to be important to check this.

# NOTE: if running things directly from the command line, I think that it is 
# going to be easy to run this code, even if doing a remote call. 
# perhaps printing to the command line is the way to go, easy 

# TODO: it should be easy to set sweeps with respect to some parameters.
# I think that it is, actually.

# TODO: using inspect, it is possible to run a function or at least to 
# get the arguments for a function.

# easy to store sequences of sequences? json files for that.

# TODO: add in the run script the option to only run if it does not 
# exist already. I think that results.json is a good way of going about it, 
# but probably not quite. the question is where should we test if something should
# be done or not? that is, to run or not.

# running those that have not been ran yet. that sounds good.
# also, maybe run guarded experiments for these. I don't think that it should 
# take too much time.

# what is a good description of a featurizer. I think that the definition of 
# the data is relatively stable, but featurizers are malleable, and depend 
# on design choices. the rest is mostly independent from design choices.

# binary search like featurization. think about featurization for 
# directory like structures.
# this is binarization. check how they do it in scikit-learn.

# manipulating the code should be easy because we can solve the problem
# by 

# download certain files periodically or when it terminates. this allows to 
# give insight into what is happening locally.
# there is syncing functionality 

# TODO: make it easy to use the sync command.

# managing jobs after they are running in the server.

# TODO: write a file with information about the system that did the run.
# gathering information about the system is important

# TODO: have a way of knowing which jobs are running on the server (i.e., which 
# are mine.) or keep this information locally. this can be done asyncronously.
# keep this information somewhere. 

# TODO: and perhaps kill some of the jobs, or have some other file 
# that say ongoing. define what is reasonable to keep there.

# NOTE: make a single experiment. it does not make sense to have multiple 
# experiments with the same configuration. easier to keep track this way
# up to three word descriptive names.

# 09-06-2017_....
# labelling experiments like this can work, I think.

# TODO:
# Calendar
# get day, get hour, get minute, get year, get month, for some, there is the 
# possibility of asking for 
# get_now(...): 
# choose whether you want as a string or number.

# the question is when the experiment is created. there should exist a single
# experiment. run_only_if_notexists. there is the question of running the code.
# this is easy, check 
# there is some. has an option to override. 
# test if a given file exists, if yes, then it does not run the experiment.
# NOTE: making everything index to the place where it is is nice.

# NOTE: to control randomness, it is necessary to set it on every call to the 
# function. that is non-trivial. look at what sacred does.

# TODO: have a simple way of syncing the folder for running experiments 
# somewhere. it should be really easy to sync the experiments on those folders.

# TODO: perhaps sync remote folder through the command line.
# it is sort of a link between two folders.

# I don't think that the current one is correct.

# find a nice way of keeping track of the experiments having a certain 
# configuration. it helps thinking about what I am doing.

# TODO: use cases about making things work. 

# sync two folders or multiple folders

# creation of all the models 

# do something about the script to get it to work 
# just do a loop, and fix the number of tasks and keep things there.

# only keep the arguments that matter in writing this thing to disk.

# it is hard to draw conclusions from random experiments. 
# typical parameters to vary are 
# computational capacity.
# amount of data
# time budget.

# TODO: add stuff to use hyperparameters optimization really easily
# stick with the str, int, float, bool types.
# NOTE that for strings, I probably have to give it a set, it is a discrete
# set of models.

# there is a question about the run script, and what can be done
# register a daemon, do something periodically, for example, reads the folder 
# there are gets the files.

# there is stuff that we can do with rsync. there is stuff that I can ignore.

# TODO: there is stuff that is part of the analysis, like the 

# NOTE: in syncing folders, it is important to keep the most recent ones
# mostly from the server from.
 
# should be easy to run multiple commands remotely.

# NOTE: it is  easy to define a function locally and then update it to the
# server.

# testing code for interacting with the server is nice because I can stop if 
# it fails to reach it, so I can iterate fast.

# NOTE: it mostly makes sense to get stuff from the server to the 
# local host.

# basically there is stuff on the upload link, and there is stuff on the 
# download link. on the upload link, I just want to sync some of the 
# experiments.

# NOTE: I think that for the most part, I will be syncing folders in the 
# same server. they may be different across experiments, but that is not 
# necesarily true.

# TODO: think about introducing the configurations in a way that they are 
# related.

# how to keep different experiments. also, the fact that the code sort of 
# changes with the time the model is run is also something important. 
# the experiments folder can be done 

# the only way of getting this for sure is to have a copy of the code at the 
# moment it is done.

# the expectations about running about running and what can I do regarding 
# different things that make a difference. for example, what is the interest 
# in having replication in the configs? maybe there is no interest, but there 
# is a question of entry point, I guess. not even that, the entry point is 
# the same in both cases. It would be better to have just one or the other
# I think that the config would be better.

# why would you change an experiment configuration. only if it was due to 
# bug. what if the code changes, well, you have to change the other experiments
# according to make sure that you can still run the exact same code, or maybe 
# not. 

# it does not make much sense, because they may become very outdated.

# the fact that I have to run a command on a server makes it that I have to 
# generate a file many times.

# TODO: add to the toolbox functionality to tune the step size.

### older stuff that has been moved.

# TODO: difference between set and define.
# set if only exists.
# define only if it does not exist.
# both should have concrete ways of doing things.

# TODO: perhaps it is possible to run the model in such a way that makes 
# it easy to wait for the end of the process, and then just gets the results.
# for example, it puts it on the server, sends the command, and waits for 
# it to terminate. gets the data everytime.

# it has to connect for this...

# TODO: the question is which are the cpus that are free. 
# NOTE: it is a good idea to think about one ssh call as being one remote 
# function call.

## TODO: get to the point were I can just push something to the server 
# and it would work.

# TODO: perhaps it is possible to get things to stop if one of the 
# them is not up.
# I'm sure that this is going to fail multiple times.

# these tree wise dictionaries

# this is nice for dictionary exploration

# TODO: it is probably a good idea to keep information about the model.

# ideally, you just want to run things once.
# also, perhaps, I would like to just point at the folder and have it work.

# TODO: develop functions to look at the most recent experiments, or the files
# that were change most recently.

# also stuff to look into the toolbox.
# the plug and play to make the experience 

# check multiples of two, powers of two, different powers too.

# stuff for adapting the step size. that is nice.

# remember that all these are ran locally.

# the commands are only pushed to the server for execution.

# easy to do subparsers and stuff, as we can always do something to
# put the subparsers in the same state are the others.
# can use these effectively.

# TODO: add something to run periodically. for example, to query the server
# or to sync folders. this would be useful.

# TODO: split these commands in parts that I can use individually.
# for example, I think that it is a good idea to 
# split into smaller files. 
# 3-7; not too many though.

# TODO: it is possible to have something that sync folders
# between two remote hosts, but it would require using the 
# current computer for that.

# TODO: get it to a point where I can call everything from the command line.

# to extract what I need to get, I only need to to use the model
# very simply.

# TODO: understand better what is the pseudo tty about.

# add error checking 

# TODO: check output or make sure that things are working the correct way.
# it may be preferable to do things through ssh sometimes, rather than 
# use some of the subprocess with ssh.

# NOTE: it is important to be on the lookout for some problems with the 
# current form of the model. it may happen that things are not properly 
# between brackets. this part is important

# there is questions about being interactive querying what there is still 
# to do there.

# there is only part of the model that it is no up. this is better.

# there is more stuff that can go into the toolbox, but for now, I think that 
# this is sufficient to do what I want to do.

# add stuff for error analysis.

# TODO: functionality for periodically running some function.

# TODO: more functionality to deal with the experiment folders.

# for reading and writiing csv files, it is interesting to consider the existing 
# csv functionality in python

# look at interfacing nicely with pandas for some dataframe preprocessing.  

# NOTE: neat tools for packing and unpacking are needed. this is necessary 
# to handle this information easily.

# TODO: mapping the experiment folder can perhaps be done differently as this is 
# not very interesting. 

# dealing with multiple dictionaries without merging them.

# going over something and creating a list out of it through function 
# calls, I think that is the best way of going.

# TODO: a recursive iterator.
# recursive map.

# NOTE: a list is like a nested dictionary with indices, so it 
# should work the same way.

# NOTE: for flatten and stuff like that, I can add some extra parts to the model
# that should work nicely, for example, whether it is a list of lists 
# or not. that is nicer.

# there are also iterators, can be done directly. this is valid, like 
# [][][][][]; returns a tuple of that form. (s, s, ... )
# this should work nicely.

# question about iterator and map, 
# I think that based on the iterator, I can do the map, but it is probably 
# a bit inefficient. I think that 

# the important thing is copying the structure.

# NOTE: some of these recursive functions can be done with a recursive map.

# most support for dictionaries and list and nested mixtures of both.
# although, I think that dictionaries are more useful.

# <IMPORTANT> TODO: it would be nice to register a set of function to validate that 
# the argument that was provided is valid. this can be done through 
# support of the interface. improve command line interface.

# TODO: make it easier to transfer a whole experiment folder to the server and 
# execute it right way.

# it seems premature to try a lot of different optimization parameters until
# you have seen that something works.

# develop tools for model inspection.

# TODO: some of the patterns about running somewhere and then 

# TODO: some easy interface to regular expressions.

# TODO: stuff to inspect the examples and look at the ones that have the most 
# mistakes. this easily done by providing a function

# TODO: there are problems about making this work.

# something can be done through environment variables, although I don't like 
# it very much.

# there is stuff that needs interfacing with running experiments. that is the 
# main use of this part of this toolbox.

# think about returning the node and the job id, such that I can kill those 
# job easily in case of a mistake.

# TODO: add stuff for coupled iteration. this is hard to do currently.
# think about the structure code.

# TODO: work on featurizers. this one should be simple to put together.

# NOTE: it is a question of looking at the number of mistakes of a 
# model and see how they are split between types of examples.

# or generate confusion matrices easily, perhaps with bold stuff for the largest
# off-diagonal entries.




# Working with the configs, I think that that is interesting.
# the folders do not matter so much, but I think that it is possible 
# do a mirror of a list [i:]

# TODO: can use ordered dict to write things to 

# this would work nicely, actually. add this functionality.
# no guarantees about the subtypes that are returned.
# this could be problematic, but I think that I can manage.

# TODO: the point about the checkpoints is that I can always analyse the 
# results whenever I want.

# <IMPORTANT> TODO: add more iterators, because that would be nice.

# TODO: add logging functionality that allows to inspect the decisions of the 
# model, for example, the scores that were produced or to re

# TODO: implement.

# generating two line tables. some filtering mechanisms along with 

# TODO: add ML models unit tests.
# like running for longer should improve performance.
# performance should be within some part of the other.
# do it in terms of the problems.

# <IMPORTANT> TODO: logging of different quantities for debugging.
# greatly reduce the problem.
# copy the repository somewhere, for example, the debug experiments.

# do it directly on main.

# <IMPORTANT> TODO: simple interface with scikit-learn

# unit tests are between different occurrences, or between different time steps.

# TODO: these can be improved, this is also information that can be added 
# to the dictionary without much effort.
import platform

def node_information():
    return platform.node()


# TODO: function to get a minimal description of the machine in which 
# the current model is running on. 

### TODO: check the overfit test, meaning that I can 
# 

# loss debugging. they should go to zero.
# easy way of incorporating this.

# both at the same time.

# TODO: dumb data for debugging. this is important to check that the model is working correctly.

# question about the debbuging. should be a sequence of 
# objects with some descriptive string. the object should probably also
# generate a string to print to a file.
# this makes sense and I think that it is possible.

# TODO: a logging object.
# can have a lot of state.

# NOTE: how should logging look like? it should tell the path. of the model. where is being called, and the 
# ids and stuff. 
# inspection module.

# conditions that should be verified for correctness. 
# conditions that should be verified. 

# how to inspect the results. perhaps have a good spreadsheet?

# that would be nice.

# TODO: for example, if I'm unsure about the correctness of some variable
# it would be nice to register its computation. how to do that.
# I would need to litter the code with it.

# also, can describe certain things that allow to inspect the log.
# it could a dynamic log file. have certain namespaces.

# TODO: passing a dictionary around is  a good way of registering informaiton 
# that you care about. this is not done very explicitly. using just a dictionary 
# is not a good way. perhaps reflection about the calling function would be 
# a good thing, and some ordering on what elements were called and why.

# TODO: have a function to append to a dictionary of lists.

# this registering is important though. how can you do this automatically
# that would be some form of what

# using nested dictionarie s 

# with the parameters that determine computation to be very small such that it
# can be ran very fast.

# runs directly from config files and result files? 
# what would one of these look like for DeepArchitect? 
# very small evaluation time, simple search space.

# TODO: add gradient checking functionality.

# TODO: add some easy way of adding tests to the experiment folders. like 
# something as to be true for all experiments.
# also something like, experiments satisfying some property, should 
# also satisfy some other property.

# NOTE: bridges is slurm managed. I assume it is only slighly different.


# do soemthing to easily register conditions to test that the experiments 
# should satisfy.

# add a non place holder for table generation.
# there must exist at most one element for the creation of this table.
# the other stuff should be common. 

# add a function to say which one should be the empty cell placeholder in the 
# table.

# TODO: stuff for error analysis. what are the main mistakes that the model 
# is doing. there should exist a simple way of filtering examples by the number
# of mistakes.

# curious how debugging of machine learning systems work. based on performance.
# differential testing.

# TODO: it should be easy to specify some composite patience clock.

# TODO: generate powers of two, powers of 10, 

# have default values for the hyperparameters that you are looking at.
# for example, for step sizes, there should exist a default search range.\

# TODO: question about debugging models

# TODO: do the featurizer and add a few common featurizers for cross product
# and bucketing and stuff.

# think about how to get the featurizer, that can be done either through 
# it's creation or it is assumed that the required information is passed as 
# argument.

# this is a simple map followed by a sort or simply sorting by the number of 
# mistakes or something like that.
# perhaps something more explicit about error analysis.

# generation of synthetic data.

# TODO: think about the implementation of some stateful elements. 
# I think that the implementation of beam search in our framework is something 
# worthy.

# simple binarization, 

# TODO: check utils to train models online. there may exist stuff that 
# can be easily used.

# TODO: simple interface with DeepArchitect.

# TODO: interface with Pandas to get feature types of something like that 
# I think that nonetheless, there is still information that needs to 
# be kept.

# one simple way of getting these models is by doing a Kaggle competition, 
# and winning it.

# TODO: stuff to manage state. for example, feature updates when something 
# changes, but otherwise, let it be.

# this can be done in the form of something that is ran when a condition is 
# satisfied, but otherwise does nothing.
# the condition may be having changes to the state and calling it
# this is lazy computation.

# how to switch once it is finished.

# tests to make the model fail very rapidly if there are bugs.
# functions to analyse those results very rapididly.

# TODO: stuff for checkpointing, whatever that means.

# add functionality to send email once stops or finishes. 
# this helps keep track of stuff.

# TODO: some easy interfacing with Pandas would be nice.
# for example to create some dictionaries and stuff like that.

# very easy to work on some of these problems, for example, 
# by having some patterns that we can use. this is interesting in terms of 
# model composition, what are typical model composition strategies?
# for example, for models with embeddings.

# TODO: functionality to work with ordered products.

# NOTE: perhaps it is possible to keep scripts out of the main library folder,
# by making a scripts folder. check if this is interesting or not.
# still, the entry point for running this code would be the main folder.
# of course, this can be changed, but perhaps it does not matter too much.

# NOTE: what about stateful feature extraction. that seems a bit more 
# tricky.

# there is the modelling aspect of the problem. what can be done.

# TODO: axis manipulation and transposition.

# TODO: online changes to configurations of the experiment. 
# this would require loading all the files, and subsituting them by some
# alteration. this is kind of like a map over the experiments folder. 
# it does not have to return anything, but it can really, I think.
# returning the prefix to the folder is the right way of doing things.

# make it easy to do the overfit tests. 
# it is a matter of passing a lot of fit functions. this can be 
# done online or by sampling a bunch of models from some cross product.

# try a linear model for debugging. try multiple.
# differential testing.

# sorting and stuff like that is a good way of 

# function to convert to ordered dict.


# NOTE: some of these aspects can be better done using the structure 
# information that we have introduced before.

# TODO: add functionality to make it easy to load models and look at it
# an analyse them.

# TODO: error inspection is something that is important to understand

# some subset of the indices can be ortho while other can be prod.

# TODO: in the config generation, it needs to be done independently for 
# each of the models. for example, the same variables may be there
# but the way they are group is different.

# TODO: for the copy update, it is not a matter of just copying
# I can also change some grouping around. for example, by grouping 
# some previously ungrouped variables.
# it is possible by rearranging the groups.
# the new grouping overrides the previous arrangement. 
# this may be tricky in cases, where the previous ones where tied.
# perhaps it is better to restrict the functionality at first.

# the simple cases are very doable, the other ones get a little more tricky.

# composite parameters that get a list of numbers are interesting, but have
# not been used much yet. it is interesting to see 
# how they are managed by the model.

# managing subparsers, like what is necessary or what is not is important
# for example, certain options are only used for certain configurations.

# TODO: add the change learning rate to the library for pytorch.

# TODO: I have to make sure that the patience counters work the way I expect 
# them to when they are reset, like what is the initial value in that case.
# for example, this makes sense in the case where what is the initial 
# number that we have to improve upon.
# if None is passed, it has to improve from the best possible.

# the different behaviors for the optimizers are nice and should work nicely 
# in practice.
# for example, the training model that we are looking at would have a 
# update to the meta model within each cost function

# for the schedules, it is not just the prev value, it is if it improves on the 
# the prev value or not.

# TODO: functionality to rearrange dictioaries, for example, by reshuffling things
# this is actually quite tricky.

# simply having different models under different parts of what we care about.

# make better use of dictionaries for functions with a variable number of 
# arguments.

# TODO: have an easy way of doing a sequence of transformations to some 
# objects. this is actually quite simple. can just just


# TODO: do some form of applying a function over some sequence, 
# while having some of the arguments fixed to some values. 
# the function needs to have all the other arguments fixed.

# TODO: also make it simple to apply a sequence of transformations 
# to the elements of a string.

# functional stuff in python to have more of the data working.
# for exmaple, so simple functions that take the data 


# think about unpacking a tuple with a single element. how is this different.

# TODO: dict updates that returns the same dictionary, such that they 
# can be sequenced easily.

# TODO: easy to build something that needs to be ran in case some condition 
# is true.

# easy to run things conditionally, like only runs a certain function 
# if some condition is true, otherwise returns the object unchanged.

# TODO: a trailling call is going to be difficult, as it is not an object in 
# general, nesting things, makes it less clear. otherwise, it can 
# pass as a sequence of things to execute. looks better.
# conditions.

# NOTE: a bunch of featurizers that take an object and compute something based 
# on that object. featurizers based on strings an 

# sequence of transformations.
# use better the vertical space.

# keep stuff like job and node id in the config.

# the toolbox about the counters needs to be more sophisticated.

# TODO: add variable time backoff rates.

# TODO: code to run a function whenever some condition is verified.
# TODO: code to handle the rates and to change the schedules whenever some 
# condition is verified.

# TODO: stuff with glob looks nice.

# TODO: an interesting thing is to have an LSTM compute an embedding for the 
# unknown words. I think that this is a good compromise between 
# efficiency and generalization.

# NOTE: this is going to be difficult.

# NOTE: some auxiliary functions that allows to easily create a set of experiments 
# there are also questions.

# stuff to append directly to the registered configs.

# partial application is sufficient to get things to work nicely.

# it is a matter of how things can be done.


## NOTE: something like this such that it just loads the weights and then 
# it is ready to apply it to data.
# class PretrainedModel:

#     def predict(self, ):
#         pass

# TODO: it is important to generate a csv with a few things and be able to look 
# at it. 
# tables are easy to look for sweeps, but it is harder to understand 
# other aspects.

# it is harder to read, but I think that with a sorting function becomes
# easier.

# each config may have a description attached.

# NOTE: some of the stuff from the plots can come out of current version of
# of the plots.

# NOTE: possibility of a adding a few common experiment structures.

# easy way of doing subexperiments, but with prefixes and stuff.

# NOTE: I think that it is probably important

# the run script for a group of experiments is going to be slighly different
# what can be done there. it would be inconvenient for large numbers to send 
# it one by one. I guess it it possible to easily do it, so not too much of a 
# problem.

# TODO: ortho increments to a current model that may yield improvements.
# or just better tools to decide on what model to check.

# I think that it is going to be important to manage the standard ways of 
# finding good hyperparameters.


### NOTE: this function can be adapter to read stuff from a column wise format 
### file, perhaps with some specification of how things should go about dealing
# with conversions of certain colunns.
# def load_data(fpath, word_cidx, tag_cidxs, lowercase):
#     with open(fpath, 'r') as f:
#         sents = []
#         tags = []
#         s = []
#         t = []        
#         for line in f:
#             line = line.strip()
#             if line == '' and len(s) > 0:
#                 sents.append(s)
#                 tags.append(t)
#                 s = []
#                 t = []
#             else:
#                 fields = line.split()
#                 if lowercase:
#                     s.append(fields[word_cidx].lower())
#                 else:
#                     s.append(fields[word_cidx])
#                 t.append( [ fields[i] for i in tag_cidxs] )
            
#     return (sents, tags)

### TODO: I think that this is a more appropriate part of the model.
# it makes more sense to keep things like this.

# # # load some of the columns in a text file.
# def load_data(fpath, idxs):
#     assert len( idxs ) > 0

#     with open(fpath, 'r') as f:
#         rs = [ [] for _ in idxs ] 

#         rs_i = [ [] for _ in idxs ]        
#         for line in f:
#             line = line.strip()
#             if line == '' and len( rs_i[0] ) > 0:

#                 # append each of the senteces to the ones already present.        
#                 for r, ri in zip(rs, rs_i):
#                     r.append(ri)

#                 rs_i = [ [] for _ in idxs ]        

#             else:
#                 fields = line.split()

#                 for i, idx in enumerate(idxs):
#                     rs_i[i].append( fields[idx] )
#     return rs

# this can be convenient to read csv files.
# reading a csv file is easy if you do not want to convert things to other 
# format.


# manipulation of the different parts of the model is important because I'm 
# not sure about how to go about it.

# TODO: handle cases with variable output depending on the arguments that are 
# passed. returning a dictionary seems like a good compromise.

# TODO: stuff to inspect the mistakes that wer

# TODO: add function to check everybody's usage on matrix. same thing for 
# lithium and matrix.

# TODO: functions to inspect the mistakes made by the model.

# NOTE: some of these consistency checks can be done automatically.

# TODO: something to randomize the training data, and to keep some set of sentences
# aligned.
# TODO: some tests like checking agreement of dimensions.
# or for paired iteration: that is pretty much just izip or something like that.

# TODO: another test that may be worth validating is that the model, 
# may have some property that must be satisfied between pairs of the models.
# using stuff in terms of dictionaries is nice.

# TODO: splitting stuff is tricky. 
# that needs to be done easily 
# st

# TODO: better way of maintaining invariants in Python.
# how can that be done.

# TODO: differential testing, given the same prediction files, are there mistakes 
# that one model makes that the other does not.
# having a function to check this is interesting.


# stuff to go back and train on all data. check this.

# NOTE: it is important to ammortize effort. is it possible to write aux functions 
# for this.

### some useful assert checks.
# NOTE: this is recursive, and should be applied only to sequences 
# of examples.

# TOOD: something that can be computed per length.

### TODO: another important thing is managing the experiments.
# this means that

# TODO: perhaps important to keep a few torch models 
# and stuff.

# TODO: the tensorflow models can be kept in a different file.

# TODO: add code to deal with the exploration of the results.
# TODO: also some tools for manipulation with LSTMs.

# TODO: stuff to build essembles easily.

# TODO: add stuff for initial step size tuning.

# TODO: add functionality to run DeepArchitect on the matrix server.


# TODO: check on 

# TODO: stuff on synthetic data.


### Categories:
# - utils
# - io
# - experiments
# - plotting
# - compute
# - analysis (this is kind like plotting)
# - system
# - optimization
# - preprocessing
# - data
# - tensorflow
# - pytorch
# - (other stuff for )
# - what about the featurizer.
# - asserts and stuff.

# TODO: find a good way of keeping these categories in a way that 
# makes sense.


# TODO: add funcitons that do high level operations.

# TODO: finally finish using 

# TODO: add function doing cross validation, that is then ran on the 
# full dataset.

# TODO: batchification

# TODO: stuff for cross validation . it has to be more general than the 
# stuff that was done in scikit-learn.

# TODO: best of three runs.

# TODO: integration with other functionality that is simple to run.

# NOTE: the worskpace gets very messy.
# the problem is dealing with the scheduling of all these parameters.
# it may be difficult.
# can have multiple schedules that require some informaiotn.
# perhaps publishing subscribing, but the only purpose is really just free the search 
# space.

# managing validation.

# are there things that are common to all models and all experiments.

# TODO: stuff to apply repeatedly the same function.

# handling multiple datasets is interesting. what can be done there?

# how to manage main files and options more effectively.

# TODO: it is interesting to think about how can the featurizers be applied to 
# a sequence context. that is more tricky. think about CRFs and stuff like that.

# TODO: think about an easy map of neural network code to C code, or even to 
# more efficient python code, but that is tricky.

# think about when does something become so slow that it turns impractical?
# 

# TODO: look into how to use multi-gpu code.

# TODO: tools for masking and batching for sequence models and stuff.
# these are interesting. 

# for example, in the case of beam search, this would imply having a pad 
# sequence.

# check mongo db or something like that, and see if it is worth it.

# how would beam search look on tensorflow, it would require executing with the 
# current parameters, and the graph would have to be fixed.

# TODO: masking in general is quite interesting.

# possible to add words in some cases with small descriptions to the experiments.

# by default, the description can be the config name.

# the only thing that how approach changes is the way the model is trained.

# beam search only changes model training.

# is it possible to use the logger to safe information to generate results or 
# graphs.

# in tensorflow and torch, models essentially communicate through 
# variables that they pass around.

# for trading of computation and safety of predictions. 
# check that this is in fact possible. multi-pass predictions.

# TODO: an interesting interface for many things with state is the 
# step; and query functions that overall seem to capture what we care about.

# TODO: encode, decode, step, 
# I think that this is actually a good way of putting this interface.


# TODO: check what an efficient implementation in Tensorflow would look like.
# i.e., beam search.

# TODO: have something very standard to sweep learning rates. 
# what can be done here?

# TODO: rate annealing in ADAM can be useful.

# TODO: restart from the best previous point some model. 
# this is tricky to do correctly. how to hangle maintaining the model, 
# and choosing these things.

# Also, having the learning rate. this is interesting.

# TODO: review all the code and implement a new model.

# TODO: interesting techniques 

# TODO: guidelines for using some other project as a starting point 
# for research. check that you can get the performance mentioned in the paper.
# check that your changes did not change, often catatrophically, that you 
# can still get the performance in the paper.

# NOTE: it is not desirable for the rate counter to always return a step size, 
# because the optimzers have state which can be thrown away in the case of a 
# 

# NOTE: default aspects for training. what are the 
# important parts, like training until things stop decreasing.

# NOTE: writing deep learning code around the notion of a function.

# TODO: stuff to deal with unknown tokens.
# TODO: sort of the best performance of the model is when it works with 
# 

# there is all this logic about training that needs to be reused.
# this is some nice analysis there.

# TODO: notion of replay. do this for training environments.

# you have to get it out of x.

# NOTE: this is kind of like conditional execution.

# some of the stuff that I mention for building networks.

# TODO: construction with the help of map reduce.




## TODO: interface for encoder decoder models.

# TODO: stuff that handles LSTM masking and unmasking.

# NOTE: an interface that write directly to disk is actually 
# quite nice, 

# pack and unpack. functionality. keeping track of the batch dimension.

# use of map and reduce with hidden layers.

# TODO: perhaps the results can be put in the end of the model.

# padding seems to be done to some degree by the other model.

# TODO: types are inevitable. the question is can we do them without 
# being burdensome?

# NOTE: even just model checkpoints based on accuracy are a good way of going 
# about it.

# TODO: the notion of a replay settings. a set of variables that is kept track
# during various places.

# TODO: notion of driving training, and keeping some set of variables 
# around that allow me to do that.

# when some condition happens, I can execute a sequence of models.

# keeping the stuff way is a good way of not cluttering the output.

# TODO: notion of keeping enough information that allows me to reconstruct the 
# information that I care about.

# batching is quite important, and I need to do something that easily goes 
# from sequences to batches.

# there is a problem with the resets.
# once it resets, you have to consider where did it reset to, so you can 
# reduce the step size from there.
# basically, resetting gets much harder.

# TODO: basically switch between the format of column and row for sequences 
# of dictionaries. this is going to be simple. also, can just provide an 
# interface for this.

# TODO: checkpoints with resetting seem like a nice pattern to the model.

# TODO: extend this information about the model to make sure that the model
# is going to look the way

# it is going to be nice to extend the config generator to make sure that 
# things work out. for example, different number of arguments, or rather 
# just sequence multiple config generators or something like that.

# that is simple to do. it is just a matter of extending a lot of stuff.

# improve debbuging messages because this is hard to read.

# TODO: add some plotting scripts for typical things like training and 
# validation error. make sure that this is easy to do from the checkpoints and 
# stuff.

# TODO: perhaps for the most part, these models could be independent.
# this means that they would not be a big deal. this is going to be interesting.

# also, the notion of copying something or serializing it to disk is relevant.

# managing a lot of hyperparameters that keep growing is troublesome.
# might also be a good idea to use a tuner. 

# check on how to get a simple interface to 

# TODO: this is going 

# TODO: clean for file creation and deletion, this may be useful to group 
# common operations.

# TODO: registering various clean up operations and stuff like that.
# that can be done at teh 

# TODO: the replay of these things can be done skipping those that 
# were skipped, because those are eseentially wasted computation.
# there is an implied schedule from the replay.

# TODO: tests through config validation.

# TODO: possibility of running multiple times and getting the median.

# TODO: have stuff for pytorch.

# TODO: what is a good way of looking at different results of the model.
# it would be interesting to consider the case 

# TODO: those runs about rate patiences are the right way of doing things.
# TODO: add easy support for running once and then running much slower.

# TODO: rates make a large difference.

# TODO: add stuff that forces the reloading of all modules.

# TODO: equivalence between training settings.

# TODO: also, for exploration of different models. what is important to 
# check layer by layer.

# TODO: visualization of different parts of the model, and statistics. this is 
# going to be interesting. like exploring how different parts of the model. 
# change with training time.

# NOTE: things that you do once per epoch do not really matter, as they will be 
# probably very cheap computationally comparing to the epoch.

# TODO: ways of training the model with different ways.

# TODO: support for differential programming trained end to end.

# TODO: even to find a good configuration, it should be possible to figure 
# out reasonable configurations by hand.

# TODO: memory augmented with pointer networks.

# TODO: for DeepArchitect is going to be done through notion of functional
# programming.

# TODO: for hooks with forward and backward. 

# TODO: for DeepArchitect should be simple to 

# TODO: check that LSTM is going. check the sequence. variable length 
# packed sequence. what does it do?

# equivalence tests. this running multiple configs that you know that 
# should give the same result.

# TODO: it may be worth to come with some form of prefix code for the experiments
# it may be worth to sort them according to the most recent.

# TODO: using matplotlib animation can be something interesting to do.

# What kind of animation would be relevant. what are the interesting things 
# to animate.

# TODO: inteegrate attention mechanisms in DeepArchitect.

# TODO: some stuff to do an ensemble. it should be simple. just average the 
# predictions of the top models, or do some form of weighting.

# TODO: just analysing the mistakes is going to be interesting. also, it should
# be easy to train and validate on the true metrics. just change a bit deep 
# architect.

# it should be trivial to do, perhaps stack a few of these images, or tie
# the weights completely. maybe not necessary.

# TODO: processing the data is going to be interesting.

# throw some decent model at it. TODO: perhaps do a simple way of paralleizing
# MCTS with thompson sampling, or something like that. what should be 
# the posterior. it can be non-parametric, I guess, although it depends on 

# TODO: stuff for easily doing ensembles of models.

# TODO: multi gpu implementation, but this is mostly data parallel.

# TODO: add support for step size tuning.

# TODO: make io checkpointers that are a little smarter about when to save
# A model to disk. only save if it it has improved or it is going down.

# if just based on full predictions, that is fine, but if transitioning 
# requires running the model, it gets a bit more complicated. multi-gpu 
# options are possible, and doable. it is a matter of model serving.

# also, the multi-gpu aspect makes it possible.

# TODO: throw away 5 percent of the data where you make the most mistakes and 
# retrain. perhaps these are training mistakes.

# transpose a nested dict.

# TODO: several step functions to featurize, and perhaps visualize the 
# results of that model.

# TODO: add features for architectures easily.

# TODO: totally add stuff for cross validation.
# use the same train and test splits.
# TODO: perhaps base it around iterables.

# TODO: do train_dev split in a reasonable way.
# perhaps with indices. how did I do it in CONLL-2000 integrate that.

# TODO: for the seed add support for other things that may have independent 
# seeds.

# TODO: can add hyperparameters for the data loading too. it 
# it just get more complicated.

# TODO: in DeepArchitect stop as soon as things go to nan.
# this is going to be clear in the loss function.

# TODO: look at ways of making this work nicely. for the case of hyperparameter 
# tuning.

# TODO: add tools for the lightweight toolbox.

# TODO: perhaps add some installation information.

# TODO: also stuff to transfer between 

# TODO: introduce some functions that can use the information efficeiently.

# TODO: long range dependencies. LSTM vs convolutional networks.

# TODO: highway networks and stuff. dynamic 

# TODO: searching over attention mechanisms.

# NOTE: something that is not very well done is differential 
# tables. it should be easy, right? it should be character by character.

# NOTE: add stuff for uniform and logspace.

# TODO: look at writing reentrant code based on checkpoint.json

# TODO: allow aliasing in some configurations. what is the right way 
# of going about this. I can be done in the experiments preprocessing part.

# NOTE: the aliasing idea for configs is quite nice.

# NOTE: better logging and accumulation of information. 
# I think that this can be done through registering, and passing 
# information that uses that registered function to do something. 
# it is quite clean like this.

# TODO: stuff to maintain aliasing, or to run alias in slightly different 
# configurations; e.g., sharing some parameters, but having the other ones 
# fixed to some relevant specification.

# do the stuff for config manager.

# for managing experiments and comparing.
# to develop something for experiment management, it it is convenient to have 
# a way of interacting with them online. this would make it.

# of the shelf, reinforcement learning.

# add information for each time a logging event of a given type is called.

# also add the option to do things to the terminal. 
# it would be convenient to make sure that  I can debug a model in such a way 
# that it makes it easy to look at differential parts of the score. 
# what to print in the terminal. there is also the question about suppression, 
# in the beginning,. the probes that we can use make sense.

# it would be nice to have a way of deactivating the logging somehow.

### copied from somewhere else.
# Experiment managing
# * Easy to log everything
# * Easy to generate plots
# * Easy to extract results
# * Easy to run on multiple different machines an merge the results
# * Easy tools for extracting results and such.

# Aspects:
#     debugging 
#     logging
#     serialization
#     experiment managing
#     profiling
#     loading/writing data
#     running multiple tasks
#     wrappers for other code

#     schedules
#     validation
#     plotting
#     structures for dealing with various types of data
#     keeping checklist of things to do
#     typical transformations for text
#     batching
#     padding
#     job managing 
#     ranges
#     reading and writing configuration files
#     tools to download things from and to servers.
#     tools for batching and prediction

# for model serialization.

# think about what would be useful with deep architect.

# some parts are clearly for interactivity.

# easy to push a folder to a server and work there.
# allows you to keep alias.

# make it easy to define argument lists using only arguments that are 
# actually used.

# TODO: to get configs that have variable number of arguments, it has to be 
# possible to specify what are the options that are defining the arguments that we care about.
# only if these are set, will those arguments be available.

# it is better to do isolate the part of the model that we care about.

# net.
# net.
# enc_lstm.
# dec_lstm. 

# this is a good naming convention to make it work.

# the fact that the configs are variable will mean that I will have to pass a 
# dictionary to the create_experiment function.

# do a ptb experiment.

# some extension of the indexing capabilities of numpy is probably interesting.

# add some more functional operations that I can use for lists, dictionaries or 
# other structures.

# TODO: it should be possible to call a function with a dictionary with 
# a superset of the arguments of a function.
# if that function has defaults, use those if those elements are not in the 
# dictionary.
# this is mostly for convenience, for functions that have a lot of 
# arguments. it may be important to structure and unstructure a dictionary
# to retrieve these types of arguments, because it gets really messy.
# namespace for arguments.

# this relies on the name agreement between elements of the same model.

# call_fn(fn, kwargs)
# where kwargs has a superset of the elements there.

# the config can be structured.


# a good idea to deal with structured configs.

# TODO: it would be nice to work directly with many configs. 
# this is hard to do right now.
# especially in the interactive case.

# I think that it would be interesting to have the 

# TODO: it is important to have some functionality to deal with different 
# paths and to generate them easily.